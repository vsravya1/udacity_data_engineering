{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import configparser\n",
    "import psycopg2\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "KEY=''#Removed for security reasons\n",
    "SECRET=''#Removed for security reasons\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = KEY\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']  = SECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log-data/',\n",
       " 'log-data/2018/11/2018-11-01-events.json',\n",
       " 'log-data/2018/11/2018-11-02-events.json',\n",
       " 'log-data/2018/11/2018-11-03-events.json',\n",
       " 'log-data/2018/11/2018-11-04-events.json',\n",
       " 'log-data/2018/11/2018-11-05-events.json',\n",
       " 'log-data/2018/11/2018-11-06-events.json',\n",
       " 'log-data/2018/11/2018-11-07-events.json',\n",
       " 'log-data/2018/11/2018-11-08-events.json',\n",
       " 'log-data/2018/11/2018-11-09-events.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket=s3.Bucket('udacity-dend')\n",
    "log_data_files = [filename.key for filename in bucket.objects.filter(Prefix='log-data')]\n",
    "log_data_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in song-data in /A/A: 603\n",
      "26 * 26 * 600 = 405600 Files in song-data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['song-data/A/A/A/TRAAAAV128F421A322.json',\n",
       " 'song-data/A/A/A/TRAAABD128F429CF47.json',\n",
       " 'song-data/A/A/A/TRAAACN128F9355673.json',\n",
       " 'song-data/A/A/A/TRAAAEA128F935A30D.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_data_files = [o.key for o in bucket.objects.filter(Prefix='song-data/A/A')]\n",
    "print (\"Number of files in song-data in /A/A:\",len(song_data_files)-1)\n",
    "print (\"26 * 26 * 600 = \"+ str(26*26*600) + \" Files in song-data\")\n",
    "song_data_files[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f11e32547f0>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"s3a://\"\n",
    "song_data = input_data + 'song-data1/A/A/A/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(song_data)\n",
    "df.createOrReplaceTempView(\"song_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songs_table = spark.sql(\"\"\"\n",
    "                            SELECT sdt.song_id, sdt.title,sdt.artist_id,sdt.year,sdt.duration\n",
    "                            FROM song_data_table sdt\n",
    "                            WHERE song_id IS NOT NULL\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songs_table.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet(output_data+'songs_table/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artists_table =  spark.sql(\"\"\"\n",
    "                                SELECT DISTINCT art.artist_id, art.artist_name,art.artist_location,art.artist_latitude,art.artist_longitude\n",
    "                                FROM song_data_table art\n",
    "                                WHERE art.artist_id IS NOT NULL\n",
    "                            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artists_table.write.mode('overwrite').parquet(output_data+'artists_table/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "log_data = input_data + 'log_data/2018/11/2018-11-01-events.json'\n",
    "df_log = spark.read.json(log_data)\n",
    "\n",
    "df_log = df_log.filter(df_log.page == 'NextSong')\n",
    "df_log.createOrReplaceTempView(\"log_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "users_table = spark.sql(\"\"\"\n",
    "                            SELECT DISTINCT userT.userId as user_id, \n",
    "                            userT.firstName as first_name,\n",
    "                            userT.lastName as last_name,\n",
    "                            userT.gender as gender,\n",
    "                            userT.level as level\n",
    "                            FROM log_data_table userT\n",
    "                            WHERE userT.userId IS NOT NULL\n",
    "                        \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_data = \"s3a://output-spark-proj/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "users_table.write.mode('overwrite').parquet(output_data+'users_table/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_test_log = spark.read.parquet(output_data + 'users_table/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                            A.start_time_sub as start_time,\n",
    "                            hour(A.start_time_sub) as hour,\n",
    "                            dayofmonth(A.start_time_sub) as day,\n",
    "                            weekofyear(A.start_time_sub) as week,\n",
    "                            month(A.start_time_sub) as month,\n",
    "                            year(A.start_time_sub) as year,\n",
    "                            dayofweek(A.start_time_sub) as weekday\n",
    "                            FROM\n",
    "                            (SELECT to_timestamp(timeSt.ts/1000) as start_time_sub\n",
    "                            FROM log_data_table timeSt\n",
    "                            WHERE timeSt.ts IS NOT NULL\n",
    "                            ) A\n",
    "                        \"\"\")\n",
    "    \n",
    "    \n",
    "time_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet(output_data+'time_table/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table = spark.sql(\"\"\"\n",
    "                                SELECT monotonically_increasing_id() as songplay_id,\n",
    "                                to_timestamp(logT.ts/1000) as start_time,\n",
    "                                month(to_timestamp(logT.ts/1000)) as month,\n",
    "                                year(to_timestamp(logT.ts/1000)) as year,\n",
    "                                logT.userId as user_id,\n",
    "                                logT.level as level,\n",
    "                                songT.song_id as song_id,\n",
    "                                songT.artist_id as artist_id,\n",
    "                                logT.sessionId as session_id,\n",
    "                                logT.location as location,\n",
    "                                logT.userAgent as user_agent\n",
    "                                FROM log_data_table logT\n",
    "                                JOIN song_data_table songT on logT.artist = songT.artist_name and logT.song = songT.title\n",
    "                            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet(output_data+'songplays_table/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
